{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_corpus, load_queries_and_answers\n",
    "from retriever import BM25Retriever\n",
    "from llm_wrapper import LLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cot_step_prompt(original_question: str, \n",
    "                              accumulated_context: str, \n",
    "                              cot_so_far: list[str],\n",
    "                              max_context_tokens=32000 # Approximate token limit for context\n",
    "                             ) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt for the LLM to generate the next CoT step.\n",
    "\n",
    "    Args:\n",
    "        original_question (str): The initial multi-hop question.\n",
    "        accumulated_context (str): Text from all retrieved passages so far.\n",
    "        cot_so_far (list[str]): A list of CoT sentences generated in previous steps.\n",
    "        max_context_tokens (int): An approximate limit for the context part of the prompt\n",
    "                                  to avoid exceeding LLM input limits. Simple character count for now.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Context Management (Simple version: truncate if too long) \n",
    "    reserved_chars_for_other_parts = len(original_question) + \\\n",
    "                                     len(\"\\n\".join(cot_so_far)) + \\\n",
    "                                     500 # For instructions and formatting\n",
    "    \n",
    "    # Max chars for context based on approximate token limit\n",
    "    max_chars_for_context_in_prompt = (max_context_tokens * 3) - reserved_chars_for_other_parts # Using 3 as a char/token rough estimate\n",
    "\n",
    "    if len(accumulated_context) > max_chars_for_context_in_prompt:\n",
    "        print(f\"Warning: Truncating accumulated context from {len(accumulated_context)} to {max_chars_for_context_in_prompt} chars for CoT prompt.\")\n",
    "        context_to_include = accumulated_context[:max_chars_for_context_in_prompt] + \"...\\n[Context Truncated]\"\n",
    "    else:\n",
    "        context_to_include = accumulated_context\n",
    "    \n",
    "    # Constructing the Chain of Thought so far\n",
    "    if cot_so_far:\n",
    "        reasoning_history = \"Previous reasoning steps:\\n\" + \"\\n\".join(f\"- {step}\" for step in cot_so_far)\n",
    "    else:\n",
    "        reasoning_history = \"This is the first reasoning step.\"\n",
    "\n",
    "    # Prompt Template: It needs to be iterative. At each step, we're asking for the *next* thought.\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful assistant performing multi-step reasoning to answer a complex question.\n",
    "                You will be given a question, some retrieved context, and any previous reasoning steps.\n",
    "                Your task is to generate the *single next logical reasoning step* based on the available information to help answer the question.\n",
    "                Focus on what information is still missing or what connection needs to be made.\n",
    "\n",
    "                If you believe you have enough information from the context and previous reasoning to directly and confidently answer the original question,\n",
    "                your response should start *ONLY* with the phrase \"The answer is: \" followed by the answer. Do not add any other prefix.\n",
    "\n",
    "                Otherwise, provide only the single next reasoning sentence that helps progress towards the answer. Do not try to answer the question prematurely if you are still reasoning.\n",
    "\n",
    "                Original Question: {original_question}\n",
    "\n",
    "                Retrieved Context:\n",
    "                ---\n",
    "                {context_to_include}\n",
    "                ---\n",
    "\n",
    "                {reasoning_history}\n",
    "\n",
    "                Based on the original question, the retrieved context, and the previous reasoning steps, what is the single next reasoning step or the final answer (if known)?\n",
    "                Tip: The reasoning is a single sentences that tell you expected to be the missing information to answer the question.\n",
    "                \"\"\"\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_final_answer_prompt(original_question: str,\n",
    "                                  full_accumulated_context: str,\n",
    "                                  full_chain_of_thought: list[str],\n",
    "                                  max_context_tokens_for_final_answer=35000 # Approx token limit for context\n",
    "                                 ) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt for the LLM to generate the final answer\n",
    "    based on all gathered information (Direct Prompting style).\n",
    "\n",
    "    Args:\n",
    "        original_question (str): The initial multi-hop question.\n",
    "        full_accumulated_context (str): Text from all retrieved passages from all hops.\n",
    "        full_chain_of_thought (list[str]): All CoT sentences generated during the IR-CoT loop.\n",
    "        max_context_tokens_for_final_answer (int): Approximate token limit for the context\n",
    "                                                   and CoT part of the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Context and CoT Management (Simple version: truncate if too long)\n",
    "    combined_evidence_text = \"Reasoning Steps Taken:\\n\" + \"\\n\".join(f\"- {step}\" for step in full_chain_of_thought) + \\\n",
    "                             \"\\n\\nSupporting Retrieved Context:\\n---\\n\" + full_accumulated_context + \"\\n---\"\n",
    "\n",
    "    # Rough character-based truncation\n",
    "    reserved_chars_for_other_parts_final = len(original_question) + 300 # For instructions\n",
    "\n",
    "    max_chars_for_evidence_in_prompt = (max_context_tokens_for_final_answer * 3) - reserved_chars_for_other_parts_final\n",
    "\n",
    "    if len(combined_evidence_text) > max_chars_for_evidence_in_prompt:\n",
    "        print(f\"Warning: Truncating combined evidence from {len(combined_evidence_text)} to {max_chars_for_evidence_in_prompt} chars for final answer prompt.\")\n",
    "        evidence_to_include = combined_evidence_text[:max_chars_for_evidence_in_prompt] + \"...\\n[Evidence Truncated]\"\n",
    "    else:\n",
    "        evidence_to_include = combined_evidence_text\n",
    "\n",
    "    # Prompt Template (Direct Answer Style)\n",
    "    prompt = f\"\"\"You are an intelligent assistant. Based on the following reasoning steps and supporting context,\n",
    "                please provide a concise and direct answer to the original question.\n",
    "\n",
    "                Original Question: {original_question}\n",
    "\n",
    "                {evidence_to_include}\n",
    "\n",
    "                Based on all the information above, what is the final answer to the original question?\n",
    "                DIRECT ANSWER ONLY NO EXPLANATION('Yes','No', Name, Place, Number, etc.)\n",
    "                The Final Answer for {original_question} is: \"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_with_ircot(\n",
    "    original_question: str,\n",
    "    retriever: BM25Retriever, # Type hint for clarity, replace with your retriever class\n",
    "    llm_wrapper: LLMWrapper, # Type hint for clarity\n",
    "    max_hops: int = 3,\n",
    "    k_retrieve_per_hop: int = 2, # Number of documents to retrieve per hop\n",
    "    max_cot_step_tokens: int = 75, # Max tokens for LLM generating a CoT step\n",
    "    max_final_answer_tokens: int = 100, # Max tokens for LLM generating final answer\n",
    "    verbose: bool = True,\n",
    "    confident: bool = True,\n",
    "    ground_truth: str = None  # Add ground truth parameter for comparison\n",
    ") -> tuple[str, int, list[str], str]:\n",
    "    \"\"\"\n",
    "    Answers a question using the Interleaving Retrieval and Chain-of-Thought (IR-CoT) process.\n",
    "\n",
    "    Args:\n",
    "        original_question (str): The complex multi-hop question.\n",
    "        retriever (BM25Retriever): An initialized retriever object.\n",
    "        llm_wrapper (LLMWrapper): An initialized LLM wrapper.\n",
    "        max_hops (int): Maximum number of reason-retrieve iterations.\n",
    "        k_retrieve_per_hop (int): Number of documents to retrieve at each hop.\n",
    "        max_cot_step_tokens (int): Max new tokens for LLM when generating a CoT step.\n",
    "        max_final_answer_tokens (int): Max new tokens for LLM when generating the final answer.\n",
    "        verbose (bool): If True, prints detailed logs of the process.\n",
    "        confident (bool): If True, stops when LLM indicates final answer.\n",
    "        ground_truth (str): The correct answer for comparison (optional).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (final_answer_text, num_hops_taken, full_chain_of_thought_list, full_accumulated_context_str)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"{'STARTING IR-COT PROCESS':^60}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Original Question: {original_question}\")\n",
    "        if ground_truth:\n",
    "            print(f\"Ground Truth Answer: {ground_truth}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    accumulated_context_str = \"\"\n",
    "    full_chain_of_thought_list = []\n",
    "    num_hops_taken = 0\n",
    "    \n",
    "    # Initial Retrieval based on the original question\n",
    "    if verbose: \n",
    "        print(f\"--- Initial Retrieval (Hop 0) ---\")\n",
    "        print(f\"Query: '{original_question}'\")\n",
    "    \n",
    "    current_query_for_retrieval = original_question\n",
    "    initial_retrieved_docs = retriever.search(current_query_for_retrieval, k=k_retrieve_per_hop)\n",
    "    \n",
    "    if initial_retrieved_docs:\n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(initial_retrieved_docs)} initial documents:\")\n",
    "            for i, doc in enumerate(initial_retrieved_docs, 1):\n",
    "                print(f\"   {i}. '{doc['title']}' (Score: {doc.get('score', 'N/A'):.4f})\")\n",
    "        \n",
    "        for doc in initial_retrieved_docs:\n",
    "            accumulated_context_str += f\"Title: {doc['title']}\\\\nPassage: {doc['text']}\\\\n<endofpassage>\\\\n\"\n",
    "    else:\n",
    "        if verbose: \n",
    "            print(f\"No initial documents retrieved for query: '{current_query_for_retrieval}'\\\\n\")\n",
    "    \n",
    "    # Iterative Reason-Retrieve Loop\n",
    "    for hop_num in range(max_hops):\n",
    "        num_hops_taken = hop_num + 1\n",
    "        if verbose: \n",
    "            print(f\"--- IR-CoT Hop {num_hops_taken}/{max_hops} ---\")\n",
    "\n",
    "        # Construct CoT Step Prompt\n",
    "        cot_prompt = construct_cot_step_prompt(\n",
    "            original_question=original_question,\n",
    "            accumulated_context=accumulated_context_str,\n",
    "            cot_so_far=full_chain_of_thought_list\n",
    "        )\n",
    "        \n",
    "        # Generate Next CoT Step\n",
    "        if verbose: print(f\"Generating CoT step {num_hops_taken}...\")\n",
    "        next_cot_sentence = llm_wrapper.generate(\n",
    "            cot_prompt,\n",
    "            max_new_tokens=max_cot_step_tokens,\n",
    "            temperature=0.5 # Lower temperature for more factual CoT steps\n",
    "        )\n",
    "        if verbose: \n",
    "            print(f\"CoT Step {num_hops_taken} Output: {next_cot_sentence}\")\n",
    "        \n",
    "        full_chain_of_thought_list.append(next_cot_sentence)\n",
    "\n",
    "        # Check for Termination\n",
    "        if next_cot_sentence.lower().startswith(\"the answer is:\"):\n",
    "            final_answer_text = next_cot_sentence[len(\"the answer is:\"):].strip()\n",
    "            if verbose: \n",
    "                print(f\"LLM indicated final answer within CoT step {num_hops_taken}.\")\n",
    "                print(f\"Extracted Answer: {final_answer_text}\")\n",
    "            if confident:\n",
    "                break\n",
    "            pass \n",
    "\n",
    "        # Update Query for Next Retrieval (use the last CoT sentence)\n",
    "        current_query_for_retrieval = next_cot_sentence\n",
    "        if current_query_for_retrieval.lower().startswith(\"the answer is:\"): # Don't retrieve based on an answer\n",
    "            if verbose: print(\"CoT step was an answer, skipping retrieval for this hop.\")\n",
    "            if hop_num == max_hops -1: # If it's an answer on the last hop, we're good.\n",
    "                 break \n",
    "            else: # If it's an answer but not the last hop, let's assume we need to verify or just proceed.\n",
    "                  # For now, we'll just skip retrieval if it looks like an answer.\n",
    "                  continue\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Retrieving based on CoT step: '{current_query_for_retrieval[:100]}{'...' if len(current_query_for_retrieval) > 100 else ''}'\")\n",
    "        \n",
    "        # Retrieve New Documents\n",
    "        newly_retrieved_docs = retriever.search(current_query_for_retrieval, k=k_retrieve_per_hop)\n",
    "\n",
    "        # Accumulate Context\n",
    "        if newly_retrieved_docs:\n",
    "            added_new_context = False\n",
    "            new_docs_info = []\n",
    "            for doc in newly_retrieved_docs:\n",
    "                # Avoid re-adding identical content if title and passage match exactly.\n",
    "                doc_full_text = f\"Title: {doc['title']}\\\\nPassage: {doc['text']}\\\\n<endofpassage>\\\\n\"\n",
    "                if doc_full_text not in accumulated_context_str : # Basic check to avoid exact duplicates\n",
    "                    accumulated_context_str += doc_full_text\n",
    "                    added_new_context = True\n",
    "                    new_docs_info.append((doc['title'], doc.get('score', 'N/A'), doc['text'][:150]))\n",
    "                \n",
    "            if added_new_context and verbose:\n",
    "                print(f\"Retrieved and added {len(new_docs_info)} new documents for hop {num_hops_taken}:\")\n",
    "                for i, (title, score, preview) in enumerate(new_docs_info, 1):\n",
    "                    print(f\"   {i}. '{title}' (Score: {score:.4f})\")\n",
    "                    print(f\"       Preview: {preview}...\")\n",
    "            elif verbose:\n",
    "                print(f\"Retrieved {len(newly_retrieved_docs)} documents, but they might be duplicates or empty.\")\n",
    "        elif verbose:\n",
    "            print(f\"No new documents retrieved for hop {num_hops_taken}\")\n",
    "        \n",
    "        # TODO: Break if no new context and CoT is not progressing.\n",
    "\n",
    "    #Final Answer Generation\n",
    "    if verbose: \n",
    "        print(f\"--- Generating Final Answer (after {num_hops_taken} hops) ---\")\n",
    "    \n",
    "    final_answer_prompt_str = construct_final_answer_prompt(\n",
    "        original_question=original_question,\n",
    "        full_accumulated_context=accumulated_context_str,\n",
    "        full_chain_of_thought=full_chain_of_thought_list\n",
    "    )\n",
    "    \n",
    "    final_answer_text = llm_wrapper.generate(\n",
    "        final_answer_prompt_str,\n",
    "        max_new_tokens=max_final_answer_tokens,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"Final Generated Answer: {final_answer_text}\")\n",
    "        if ground_truth:\n",
    "            print(f\"Ground Truth Answer: {ground_truth}\")\n",
    "            print(f\"Match: {'Yes' if final_answer_text.strip().lower() == ground_truth.strip().lower() else 'No'}\")\n",
    "        print(f\"Total Hops Used: {num_hops_taken}\")\n",
    "        print(f\"CoT Steps Generated: {len(full_chain_of_thought_list)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"IR-COT PROCESS FINISHED\")\n",
    "        print(f\"{'='*60}\\\\n\")\n",
    "        \n",
    "    return final_answer_text, num_hops_taken, full_chain_of_thought_list, accumulated_context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 609 documents from corpus.\n",
      "Loaded 2556 query-answer pairs.\n",
      "Loaded 609 documents and 2556 questions.\n",
      "\n",
      "First document from corpus:\n",
      "{'id': 0, 'title': \"200+ of the best deals from Amazon's Cyber Monday sale\", 'passage': 'Table of Contents Table of Contents Echo, Fire TV, and Kindle deals Apple deals TV deals Laptop deals Headphone and earbud deals Tablet deals Gaming deals Speaker deals Vacuum deals Kitchen deals Smart home deals Fitness deals Beauty tech deals Drone deals Camera deals Lego deals Gift card deals\\n\\nUPDATE: Nov. 27, 2023, 5:00 a.m. EST This post has been updated with all of the latest Cyber Monday deals available at Amazon.\\n\\nAmazon is dragging out the year\\'s biggest shopping holiday(s) into 11 days of deals.\\n\\nThe retail giant began its Black Friday sale in the early morning of Friday, Nov. 17 (a week ahead of schedule) and was on top of making the switch to Cyber Monday language in the wee hours of Saturday, Nov. 25. Official Cyber Monday mode, which is currently on through Monday, Nov. 27, includes both a ton of deals carried over from Black Friday plus some new ones.\\n\\nWe\\'re curating a running list of Amazon\\'s best Cyber Weekend deals, spotlighting some of our favorites and noting when good deals inevitably sell out. Read on for the full rundown, and check back often: We\\'re going to be updating this story incessantly as the sale continues, as well as our even bigger (if you can imagine) list of Cyber Monday deals across more retailers.\\n\\nNote: All newly added deals are marked with a ✨, while deals with a 🔥 have dropped to an all-time low price. Amazon\\'s invite-only deals for Prime members are marked with a 📨. Deals with a strikeout were either sold out or expired at the time of writing.\\n\\nEcho, Fire TV, and Kindle deals\\n\\nWhy we like it\\n\\nAn Echo Show is a subtle yet game-changing addition to any room — and for less than $40 with this rollover Black Friday to Cyber Monday deal, there\\'s little reason to not make your life easier. The smart screen responds to Alexa commands that are particularly handy when your hands are full, like asking for measurement conversions mid-cooking, checking the weather mid-rushing out the door, or turning off your smart lights with a sleeping kid in hand. Plus, it\\'s got a 5.5-inch screen and better sound than its predecessor, making it perfect for watching videos or video calling friends and family.\\n\\nMore Amazon device and service deals\\n\\nAmazon services\\n\\nAudible Premium Plus — $5.95/month $14.95/month for four months (save $9/month; new customers only; get an additional $20 Audible credit)\\n\\nEcho Buds\\n\\nEcho smart displays\\n\\nEcho smart speakers\\n\\nFire tablets\\n\\nFire TVs\\n\\nNote: All Fire TVs come with a free 6-month subscription to MGM+ (a $35.94 value).\\n\\nFire TV streaming devices\\n\\neero\\n\\nKindles\\n\\nMiscellaneous Amazon devices\\n\\nApple deals\\n\\nWhy we like it\\n\\nNow that the 64GB 9th generation iPad has been going in and out of stock (you might be able to find it on sale for $229.99), our new favorite iPad deal at Amazon is the 10th generation 64GB model for $349. Compared to the 9th gen, the 10th gen classic iPad has a slightly bigger screen that\\'s now also a Liquid Retina display (10.9 inches versus 10.2 inches), a faster A14 Bionic chip for smoother multitasking, and USB-C charging.\\n\\nMore Apple deals\\n\\nAirPods\\n\\nMacBook\\n\\nMac\\n\\niPad\\n\\nApple Watch\\n\\nTV deals\\n\\nWhy we like it\\n\\nThis 65-inch Fire TV from Amazon hit its lowest ever price this month, and we\\'re not mad about it. With a 66 percent five-star review rating, it\\'s got much to love: Including stunning 4K QLED resolution, adaptive brightness that adjusts to the lighting of your room, the ability to project famous art or personal pics on it when not streaming anything, and, of course, that quintessential Alexa voice control.\\n\\nMore TV deals\\n\\n43 to 55 inches\\n\\n65 inches\\n\\n75 to 85 inches\\n\\nLaptop deals\\n\\nWhy we like it\\n\\nMacBooks aside (which are all listed above in the Apple section), another stellar Cyber Monday laptop deal at Amazon is the lightweight Microsoft Surface Laptop Go 3 for $599.99. This 25% discount drops the 2023 version to the regular asking price of our favorite budget laptop, the older Surface Go 2. Compared to the Go 2, the Go 3\\'s Intel Core i5 processor is 12th gen versus the Go 2\\'s 11th gen, harnessing better speeds and solid power for most everyday work or school tasks. On the outside, the Go 3 is definitely giving MacBook Air — the main difference being that the Go 3\\'s screen is a touchscreen.\\n\\nMore laptop deals\\n\\nTraditional laptops\\n\\n2-in-1 laptops\\n\\nChromebooks\\n\\nGaming laptops\\n\\nHeadphone and earbud deals\\n\\nWhy we like it\\n\\nNarrowing down a headphones upgrade from so many on-sale options is less overwhelming when you\\'ve confirmed that you want to stick with a super premium, super reputable brand like Bose — but also that you want to stick to a budget. There\\'s only one pair of Bose over-ear headphones you can grab for just under $200, and that\\'s the QuietComfort 45s at a record-low price of $199. This classic pair secures top-of-the-line ANC, 20 hours of battery life, and all-day comfort for less than half of the AirPods Max\\'s sale price.\\n\\nMore headphone and earbud deals\\n\\nHeadphones\\n\\nEarbuds\\n\\nTablet deals\\n\\nGaming deals\\n\\nGaming headsets\\n\\nMashable Deals Want more hand-picked deals in your inbox? Sign up for Mashable\\'s daily Deals newsletter. Loading... Sign Me Up By signing up you agree to our Terms of Use and Privacy Policy Thanks for signing up!\\n\\nGaming mice\\n\\nGaming keyboards\\n\\nVR headsets\\n\\nMeta Quest 2 — $249 $299.99 (save $50.99) + free $50 Amazon credit with code META50 🔥\\n\\nSpeaker deals\\n\\nVacuum deals\\n\\nCordless vacuums\\n\\nRobot vacuums and vacuum/mop hybrids\\n\\nKitchen deals\\n\\nPizza ovens\\n\\nSparkling water makers\\n\\nToaster ovens\\n\\nSmart home deals\\n\\nNote: Echo devices are listed above under \"Amazon device and service deals.\"\\n\\nBlink\\n\\nChromecast\\n\\nGoogle Nest\\n\\nFitness deals\\n\\nSmartwatches and fitness trackers\\n\\nNote: Apple Watches are listed above under \"Apple deals.\"\\n\\nBeauty tech deals\\n\\nDrone deals\\n\\nCamera deals\\n\\nGoPro\\n\\nLego deals\\n\\nGift card deals'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "CORPUS_FILEPATH = \"data\\\\multihoprag_corpus.txt\"\n",
    "QA_FILEPATH = \"data\\\\MultiHopRAG.json\"\n",
    "\n",
    "corpus_documents = load_corpus(CORPUS_FILEPATH)\n",
    "qa_dataset = load_queries_and_answers(QA_FILEPATH)\n",
    "\n",
    "print(f\"Loaded {len(corpus_documents)} documents and {len(qa_dataset)} questions.\")\n",
    "\n",
    "print(\"\\nFirst document from corpus:\")\n",
    "print(corpus_documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing retriever...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing retriever...\")\n",
    "bm25_retriever = BM25Retriever(corpus_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Ollama. Available models: ['phi4:14b', 'llama3.2:3b', 'gemma3:4b-it-qat', 'qwen3:8b']\n",
      "LLMWrapper initialized for model: gemma3:4b-it-qat (type: ollama)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "llm = LLMWrapper(\n",
    "    model_identifier=\"gemma3:4b-it-qat\",\n",
    "    llm_type=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the individual associated with both the failed crypto exchange FTX and Alameda Research, alleged to have used deceitful practices for personal gain and influence, and is facing charges of fraud and conspiracy according to articles from Fortune and TechCrunch?\n",
      "Answer: Sam Bankman-Fried\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,1000)\n",
    "\n",
    "question = qa_dataset[index]['query']\n",
    "answer = qa_dataset[index]['answer']\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "                  STARTING IR-COT PROCESS                   \n",
      "============================================================\n",
      "Original Question: Who is the individual associated with both the failed crypto exchange FTX and Alameda Research, alleged to have used deceitful practices for personal gain and influence, and is facing charges of fraud and conspiracy according to articles from Fortune and TechCrunch?\n",
      "============================================================\n",
      "--- Initial Retrieval (Hop 0) ---\n",
      "Query: 'Who is the individual associated with both the failed crypto exchange FTX and Alameda Research, alleged to have used deceitful practices for personal gain and influence, and is facing charges of fraud and conspiracy according to articles from Fortune and TechCrunch?'\n",
      "Retrieved 2 initial documents:\n",
      "   1. 'The FTX trial is bigger than Sam Bankman-Fried' (Score: 117.0476)\n",
      "   2. 'SBF’s trial starts soon, but how did he — and FTX — get here?' (Score: 116.0725)\n",
      "--- IR-CoT Hop 1/10 ---\n",
      "Generating CoT step 1...\n",
      "CoT Step 1 Output: The answer is: Based on the provided context, Sam Bankman-Fried’s alleged deceitful practices involved diverting customer funds to Alameda Research, a related company.\n",
      "LLM indicated final answer within CoT step 1.\n",
      "Extracted Answer: Based on the provided context, Sam Bankman-Fried’s alleged deceitful practices involved diverting customer funds to Alameda Research, a related company.\n",
      "--- Generating Final Answer (after 1 hops) ---\n",
      "\\n============================================================\n",
      "Final Generated Answer: Sam Bankman-Fried\n",
      "Total Hops Used: 1\n",
      "CoT Steps Generated: 1\n",
      "============================================================\n",
      "IR-COT PROCESS FINISHED\n",
      "============================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# Run IR-CoT\n",
    "predicted_answer, hops, cot_chain, retrieved_ctx = answer_question_with_ircot(\n",
    "    original_question=question,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    max_hops=10,\n",
    "    max_cot_step_tokens=1000,\n",
    "    max_final_answer_tokens=1000,\n",
    "    k_retrieve_per_hop=2,\n",
    "    verbose=True,\n",
    "    confident=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import calculate_f1_score\n",
    "from run_pipelines import run_qa_system\n",
    "from utils import load_corpus, load_queries_and_answers\n",
    "from retriever import BM25Retriever\n",
    "from llm_wrapper import LLMWrapper\n",
    "from run_evaluation import run_comprehensive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FILEPATH = \"data\\\\multihoprag_corpus.txt\"\n",
    "QA_FILEPATH = \"data\\\\MultiHopRAG.json\"\n",
    "\n",
    "corpus_docs = load_corpus(CORPUS_FILEPATH)\n",
    "if corpus_docs:\n",
    "    print(f\"\\nFirst document from corpus: {corpus_docs[0]}\")\n",
    "    print(f\"Last document from corpus: {corpus_docs[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "corpus_documents = load_corpus(CORPUS_FILEPATH)\n",
    "qa_dataset = load_queries_and_answers(QA_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing retriever...\")\n",
    "bm25_retriever = BM25Retriever(corpus_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "llm = LLMWrapper(\n",
    "    model_identifier=\"gemma3:4b-it-qat\",\n",
    "    llm_type=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"multi-hop-ircot\"\n",
    ")\n",
    "\n",
    "print(\"MULTI HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"single-hop-rag\"\n",
    ")\n",
    "\n",
    "print(\"SINGLE HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"llm-only\"\n",
    ")\n",
    "\n",
    "print(\"LLM ONLY: \")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLMWrapper(\n",
    "    model_identifier=\"qwen3:8b\",\n",
    "    llm_type=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"multi-hop-ircot\"\n",
    ")\n",
    "\n",
    "print(\"MULTI HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"single-hop-rag\"\n",
    ")\n",
    "\n",
    "print(\"SINGLE HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"llm-only\"\n",
    ")\n",
    "\n",
    "print(\"LLM ONLY: \")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLMWrapper(\n",
    "    model_identifier=\"llama3.2:3b\",\n",
    "    llm_type=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"multi-hop-ircot\"\n",
    ")\n",
    "\n",
    "print(\"MULTI HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"single-hop-rag\"\n",
    ")\n",
    "\n",
    "print(\"SINGLE HOP: \")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, summary = run_comprehensive_evaluation(\n",
    "    qa_dataset=qa_dataset,\n",
    "    retriever=bm25_retriever,\n",
    "    llm_wrapper=llm,\n",
    "    num_samples=200,\n",
    "    max_ircot_hops=10,\n",
    "    k_retrieve_multi_hop=2,\n",
    "    k_retrieve_single_hop=2,\n",
    "    verbose=True,\n",
    "    verbose_level=0,\n",
    "    run_mode=\"llm-only\"\n",
    ")\n",
    "\n",
    "print(\"LLM ONLY: \")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
